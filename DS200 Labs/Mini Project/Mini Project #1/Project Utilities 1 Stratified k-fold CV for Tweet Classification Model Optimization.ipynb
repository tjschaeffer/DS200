{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### PSU DS 200  Fall 2019\n\n#### Project Utility Jupyter Notebook 1\nThis Jupyter Notebook is intended to be used to construct Project Deliverables for the Mini-project\nof DS 200. \n#### Acknowledgement: The datascience module is developed by University of California Berkeley for its data 8 course.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install datascience",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting datascience\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/c1/fff066029adeaaafe12d59a906440da8ce2dcffc4cf852e9474b80a04dad/datascience-0.15.3.tar.gz (42kB)\n\u001b[K     |████████████████████████████████| 51kB 99kB/s  eta 0:00:01\n\u001b[?25hCollecting folium>=0.9.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/ff/004bfe344150a064e558cb2aedeaa02ecbf75e60e148a55a9198f0c41765/folium-0.10.0-py2.py3-none-any.whl (91kB)\n\u001b[K     |████████████████████████████████| 92kB 280kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: sphinx in /home/nbuser/anaconda3_420/lib/python3.5/site-packages/Sphinx-1.4.6-py3.5.egg (from datascience) (1.4.6)\nRequirement already satisfied: setuptools in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (41.6.0)\nCollecting matplotlib>=3.0.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/61/465fb3bfba684b0f53b5c4829c3c89e86e6fe9fdcdfda93e38f1788090f0/matplotlib-3.0.3-cp35-cp35m-manylinux1_x86_64.whl (13.0MB)\n\u001b[K     |████████████████████████████████| 13.0MB 18kB/s  eta 0:00:01    |██████████████████▎             | 7.4MB 2.6MB/s eta 0:00:03     |████████████████████████▌       | 10.0MB 959kB/s eta 0:00:04\n\u001b[?25hRequirement already satisfied: pandas in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (0.19.2)\nRequirement already satisfied: scipy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (1.1.0)\nRequirement already satisfied: numpy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (1.17.3)\nRequirement already satisfied: ipython in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (6.2.1)\nRequirement already satisfied: pytest in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (2.9.2)\nCollecting coverage==4.5.3\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/d1/df283ea8e30aff9d567b111f40184260caa4a54f55aa26b48b82cf79161e/coverage-4.5.3-cp35-cp35m-manylinux1_x86_64.whl (205kB)\n\u001b[K     |████████████████████████████████| 215kB 9.0MB/s eta 0:00:01\n\u001b[?25hCollecting coveralls\n  Downloading https://files.pythonhosted.org/packages/b5/6a/bd33b20b03eb8d596e6b7ccfea66ca3b85fadf55ae8e6086091f498fc3d6/coveralls-1.8.2-py2.py3-none-any.whl\nRequirement already satisfied: bokeh in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from datascience) (0.12.7)\nCollecting jinja2>=2.9\n  Using cached https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl\nRequirement already satisfied: requests in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from folium>=0.9.1->datascience) (2.14.2)\nCollecting branca>=0.3.0\n  Downloading https://files.pythonhosted.org/packages/63/36/1c93318e9653f4e414a2e0c3b98fc898b4970e939afeedeee6075dd3b703/branca-0.3.1-py3-none-any.whl\nRequirement already satisfied: six>=1.5 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (1.11.0)\nRequirement already satisfied: Pygments>=2.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (2.1.3)\nRequirement already satisfied: docutils>=0.11 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (0.12)\nRequirement already satisfied: snowballstemmer>=1.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (1.2.1)\nRequirement already satisfied: babel!=2.0,>=1.3 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (2.3.4)\nRequirement already satisfied: alabaster<0.8,>=0.7 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (0.7.9)\nRequirement already satisfied: imagesize in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from sphinx->datascience) (0.7.1)\nCollecting kiwisolver>=1.0.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/18/4cd2e84c6aff0c6a50479118083d20b9e676e5175a913c0ea76d700fc244/kiwisolver-1.1.0-cp35-cp35m-manylinux1_x86_64.whl (90kB)\n\u001b[K     |████████████████████████████████| 92kB 602kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from matplotlib>=3.0.0->datascience) (2.8.1)\nRequirement already satisfied: cycler>=0.10 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from matplotlib>=3.0.0->datascience) (0.10.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from matplotlib>=3.0.0->datascience) (2.1.4)\nRequirement already satisfied: pytz>=2011k in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pandas->datascience) (2016.6.1)\nRequirement already satisfied: jedi>=0.10 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (0.11.0)\nRequirement already satisfied: decorator in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (4.4.1)\nRequirement already satisfied: pickleshare in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (0.7.4)\nRequirement already satisfied: simplegeneric>0.8 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (0.8.1)\nRequirement already satisfied: traitlets>=4.2 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (4.3.1)\nRequirement already satisfied: prompt_toolkit<2.0.0,>=1.0.4 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (1.0.15)\nRequirement already satisfied: pexpect in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from ipython->datascience) (4.0.1)\nRequirement already satisfied: py>=1.4.29 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pytest->datascience) (1.4.31)\nCollecting docopt>=0.6.1\n  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\nRequirement already satisfied: PyYAML>=3.10 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from bokeh->datascience) (3.13)\nRequirement already satisfied: tornado>=4.3 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from bokeh->datascience) (4.4.1)\nRequirement already satisfied: bkcharts>=0.2 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from bokeh->datascience) (0.2)\nRequirement already satisfied: MarkupSafe>=0.23 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from jinja2>=2.9->folium>=0.9.1->datascience) (0.23)\nRequirement already satisfied: parso==0.1.* in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from jedi>=0.10->ipython->datascience) (0.1.1)\nRequirement already satisfied: wcwidth in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from prompt_toolkit<2.0.0,>=1.0.4->ipython->datascience) (0.1.7)\nBuilding wheels for collected packages: datascience, docopt\n  Building wheel for datascience (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for datascience: filename=datascience-0.15.3-cp35-none-any.whl size=44582 sha256=b39bdbc015d5c37c2006b29eea70825f68d548a1fdb537a6490f9ea59279f37e\n  Stored in directory: /home/nbuser/.cache/pip/wheels/b8/37/0a/80274866028f6485c5957f0e1acf8e2b755fbe9dd0fd4ad275\n  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=19851 sha256=f9e7de27b985ed77163c15dcf1d032af55ea17454ea0a21a3dee3f7e87a2441e\n  Stored in directory: /home/nbuser/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\nSuccessfully built datascience docopt\nInstalling collected packages: jinja2, branca, folium, kiwisolver, matplotlib, coverage, docopt, coveralls, datascience\n  Found existing installation: Jinja2 2.8\n    Uninstalling Jinja2-2.8:\n      Successfully uninstalled Jinja2-2.8\n  Found existing installation: matplotlib 2.1.1\n    Uninstalling matplotlib-2.1.1:\n      Successfully uninstalled matplotlib-2.1.1\nSuccessfully installed branca-0.3.1 coverage-4.5.3 coveralls-1.8.2 datascience-0.15.3 docopt-0.6.2 folium-0.10.0 jinja2-2.10.3 kiwisolver-1.1.0 matplotlib-3.0.3\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import datascience\nimport numpy as np\nimport graphviz\n\nfrom datascience import *\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics \n\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.externals import joblib\n\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:232: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "t1 = Table.read_table(\"LabelledTweets.csv\", sep =',')\nt1.show(10)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "File b'LabelledTweets.csv' does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7c95d0a064c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LabelledTweets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/datascience/tables.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(cls, filepath_or_buffer, *args, **vargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File b'LabelledTweets.csv' does not exist"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The code below extract the \"class label\" from the CSV file to find out the total number of positive/supportive vs negative/non-supportive tweets in the labelled data set.\n### If you use Climage Change tweets, the column label for the class label is 'Support'.\n### If you use Airline Sentiment tweets, the column label for the class label is 'airline_sentiment' (as shown in Lab9)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "t1_positive = t1.where('Support', are.equal_to(1))\nt1_negative = t1.where('Support', are.equal_to(0))\nt1_positive",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "t1_negative",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = t1_positive.append(t1_negative)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Use the data for constructing a Decision Tree Stance Classifier\ndata_tagged_X= list(data['Text'])\ndata_tagged_Y= list(data['Support'])\n\nprint('tagged data input size', len(data_tagged_X))\nprint('tagged data target prediction size', len(data_tagged_Y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Stratified KFold Cross Validation\nStratifiedKFold is a function in sklearn under the submodule model_selection.\n#### Notice: In the beginning of this notebook, we import StratifiedKFold using the following python code:\n    from sklearn.model_selection import StratifiedKFold\n    \nThe n_splits parameter of StratifiedKFold indicates how many folds to use.  For example, \nn_splits=5 means Stratified 5-fold cross validation.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Use Stratified Kfold Cross Validation so that\n#   each fold contains the same ratio of positive/negative instances\nk = ???\nskf = StratifiedKFold(n_splits= k , random_state=1, shuffle= True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "### Set the range of max_depth for finding the best max_depth for this problem\nlow_max_depth = ???\nhigh_max_depth = ???\ntraining_performance_table = np.empty( [high_max_depth+1, 10, 5] )\ntesting_performance_table = np.empty( [high_max_depth+1, 10, 5] )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for depth in range(low_max_depth, high_max_depth+1):\n\n    fold = 1\n\n    for train_index, test_index in skf.split(data_tagged_X, data_tagged_Y):\n        print(\"Fold Number:\", fold)\n     #   print(\"Training Data Index:\", train_index)\n        print(\"Testing Data Index:\", test_index)  \n    \n        x_train= list(data.take(train_index)['Text'])     \n    #    print(\"Training Data:\", x_train)\n        y_train= list(data.take(train_index)['Support'])\n    #    print(\"Training Data Target Output:\", y_train)\n        x_test= list(data.take(test_index)['Text'])\n    #    print(\"Testing Data:\", x_test)\n        y_test= list(data.take(test_index)['Support'])\n        print(\"Testing Data Target Output:\", y_test)\n    \n        count_vect = CountVectorizer(token_pattern='((?:([@#]|[0-9]|[a-z]|[A-Z])+))', analyzer= 'word', min_df=2)\n        X_word_vect = count_vect.fit_transform(x_train)\n    \n        clf = tree.DecisionTreeClassifier(criterion='entropy', random_state = 100, max_depth=depth, \\\n                                  min_samples_leaf =2)\n    \n        clf.fit(X_word_vect, y_train)\n    \n        ### Use the model generated to predict for training data\n        predicted_training_y = clf.predict(X_word_vect)\n    \n        train_p = metrics.precision_score(y_train, predicted_training_y)\n        train_r = metrics.recall_score(y_train, predicted_training_y)\n        train_f1= metrics.f1_score(y_train, predicted_training_y)\n        training_performance_table[depth, fold, 0]=depth\n        training_performance_table[depth, fold, 1]=train_p\n        training_performance_table[depth, fold, 2]=train_r\n        training_performance_table[depth, fold, 3]=train_f1\n        print(\"Max depth is \", depth, \"Prediction Performance for Training Data f1:\", train_f1)\n\n        ### Use the model generated to predict for testint data\n        x_test_word_vect = count_vect.transform(x_test)\n        predicted_testing_y = clf.predict(x_test_word_vect)\n    \n        test_p= metrics.precision_score(y_test, predicted_testing_y)\n        test_r= metrics.recall_score(y_test, predicted_testing_y)\n        test_f1 = metrics.f1_score(y_test, predicted_testing_y)\n        testing_performance_table[depth, fold, 0]=depth\n        testing_performance_table[depth, fold, 1]=test_p\n        testing_performance_table[depth, fold, 2]= test_r\n        testing_performance_table[depth, fold, 3]= test_f1\n        print(\"Prediction Performance for Testing Data f1:\", test_f1)\n    \n        fold=fold+1\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Calculate Average, Minimum, and Maximum f1 score across folds \nThe first dimension of training_performance_table refers to a specific max_depth value.\nThe second dimension of training_performance_table refers to the number of a fold (in a k-fold).\nThe third dimension of training_performance_table saves f1 score in index \"3\".\n\nFor example, the code below returns an array of f1_score for all k folds for max_depth set to 7."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "f1_array=training_performance_table[7, 1:k+1:1, 3]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Numpy offers a convenient way to calculate the average, the minimum, and the maximum value of a given array:\n    np.average returns the average of a given array\n    np.amin returns the minimum value of a given array\n    np.amax regturns the maximum value of a given array"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.average(f1_array)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.amin(training_performance_table[7, 1:k+1:1, 3])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.amax(training_performance_table[7, 1:k+1:1, 3])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "### We want to calculate the average performance across all folds for each max_depth so that we can compare them. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cv_training_f1_summary = np.empty( [high_max_depth+1, 3])\ncv_testing_f1_summary = np.empty( [high_max_depth+1, 3])\nfirst_fold = 1\nlast_fold = 5+1\nfor depth in range(low_max_depth, high_max_depth+1):\n    train_f1_array = training_performance_table[depth, first_fold:last_fold:1 , 3]\n    cv_training_f1_summary[depth, 0] = np.average(train_f1_array)\n    cv_training_f1_summary[depth, 1] = np.amin(train_f1_array)\n    cv_training_f1_summary[depth, 2] = np.amax(train_f1_array)\n    test_f1_array = testing_performance_table[depth, first_fold:last_fold:1, 3]\n    cv_testing_f1_summary[depth, 0] = np.average(test_f1_array)\n    cv_testing_f1_summary[depth, 1] = np.amin(test_f1_array)\n    cv_testing_f1_summary[depth, 2] = np.amax(test_f1_array)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### We want to plot the average of f1 scores (across all k folds) for each max_depth, and compare the average of f1 scores for training data and testing data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import *",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "matplotlib.use('Agg', warn=False)\n%matplotlib inline\nimport matplotlib.pyplot as plots\nplots.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "depth=np.linspace(low_max_depth, high_max_depth, high_max_depth-low_max_depth+1)\nprint(depth)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "avg_f1_training = cv_training_f1_summary[low_max_depth:high_max_depth+1, 0]\nmin_f1_training = cv_training_f1_summary[low_max_depth:high_max_depth+1, 1]\navg_f1_testing = cv_testing_f1_summary[low_max_depth:high_max_depth+1, 0]\nmin_f1_testing = cv_testing_f1_summary[low_max_depth:high_max_depth+1, 1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The following code is for plotting average f1 (across k folds)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.plot(depth, avg_f1_training, label=\"? fold CV Avg f1 for training\")\nplt.plot(depth, avg_f1_testing, label=\"? fold CV Avg f1 for testing\")\nplt.xlabel('max_depth')\nplt.title('? Fold CV Avg f1 vs max_depth for trainiong data and testing data')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.plot(depth, avg_f1_training, label=\"? fold CV Avg f1 for training\")\nplt.plot(depth, min_f1_training, label=\"? fold CV Min f1 for training\")\nplt.plot(depth, avg_f1_testing, label=\"? fold CV Avg f1 for testing\")\nplt.plot(depth, min_f1_testing, label=\"? fold CV Min f1 for testing\")\nplt.xlabel('max_depth')\nplt.title('? Fold CV Avg and Min f1 vs max_depth for trainiong data and testing data')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}